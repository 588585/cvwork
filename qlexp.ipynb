{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all code from https://www.datacamp.com/tutorial/introduction-q-learning-beginner-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pyglet==1.5.1\n",
    "!apt install python-opengl\n",
    "!apt install ffmpeg\n",
    "!apt install xvfb\n",
    "!pip3 install pyvirtualdisplay\n",
    "\n",
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install gym==0.24\n",
    "!pip install pygame\n",
    "!pip install numpy\n",
    "\n",
    "!pip install imageio imageio_ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import imageio\n",
    "from tqdm.notebook import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space Discrete(16)\n",
      "Sample observation 5\n",
      "Action Space Shape 4\n",
      "Action Space Sample 3\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\",map_name=\"4x4\",is_slippery=False)\n",
    "\n",
    "print(\"Observation Space\", env.observation_space)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # display a random observation\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "print(\"Action Space Sample\", env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  16  possible states\n",
      "There are  4  possible actions\n"
     ]
    }
   ],
   "source": [
    "state_space = env.observation_space.n\n",
    "print(\"There are \", state_space, \" possible states\")\n",
    "\n",
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_q_table(state_space, action_space):\n",
    "  Qtable = np.zeros((state_space, action_space))\n",
    "  return Qtable\n",
    "\n",
    "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "  random_int = random.uniform(0,1)\n",
    "  if random_int > epsilon:\n",
    "    action = np.argmax(Qtable[state])\n",
    "  else:\n",
    "    action = env.action_space.sample()\n",
    "  return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(Qtable, state):\n",
    "  action = np.argmax(Qtable[state])\n",
    "  return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_training_episodes = 1000\n",
    "learning_rate = 0.7        \n",
    "\n",
    "# Evaluation parameters\n",
    "n_eval_episodes = 100      \n",
    "\n",
    "# Environment parameters\n",
    "env_id = \"FrozenLake-v1\"   \n",
    "max_steps = 99             \n",
    "gamma = 0.95               \n",
    "eval_seed = []             \n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0           \n",
    "min_epsilon = 0.05           \n",
    "decay_rate = 0.0005           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "  for episode in trange(n_training_episodes):\n",
    " \n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "\n",
    "    # repeat\n",
    "    for step in range(max_steps):\n",
    "   \n",
    "      action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "\n",
    "   \n",
    "      new_state, reward, done, info = env.step(action)\n",
    "\n",
    "   \n",
    "      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
    "\n",
    "      # If done, finish the episode\n",
    "      if done:\n",
    "        break\n",
    "     \n",
    "      # Our state is the new state\n",
    "      state = new_state\n",
    "  return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c219db6619c643ffbae9712c34ab309d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.73509189, 0.77378094, 0.77378094, 0.73509189],\n",
       "       [0.73509189, 0.        , 0.81450625, 0.77378094],\n",
       "       [0.77378094, 0.857375  , 0.77378094, 0.81450625],\n",
       "       [0.81450625, 0.        , 0.77378094, 0.77378094],\n",
       "       [0.77378094, 0.81450625, 0.        , 0.73509189],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9025    , 0.        , 0.81450625],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.81450625, 0.        , 0.857375  , 0.77378094],\n",
       "       [0.81450625, 0.9025    , 0.9025    , 0.        ],\n",
       "       [0.857375  , 0.95      , 0.        , 0.857375  ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9025    , 0.95      , 0.857375  ],\n",
       "       [0.9025    , 0.95      , 1.        , 0.9025    ],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)\n",
    "Qtable_frozenlake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space MultiDiscrete([4 4 4 4])\n",
      "Action Space Shape 4\n",
      "There are  MultiDiscrete([4 4 4 4])  possible states\n",
      "There are  4  possible actions\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import envs\n",
    "import time\n",
    "import pyautogui\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\jim\\\\git\\\\cvwork\\\\src\\\\gym-maze')\n",
    "import gym_maze\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  # avoid non-GUI warning for matplotlib\n",
    "from IPython.display import display, HTML\n",
    "from tqdm.notebook import trange\n",
    "from gym_maze.envs import MazeEnv\n",
    "from gym_maze.envs.generators import SimpleMazeGenerator, RandomMazeGenerator, RandomBlockMazeGenerator, \\\n",
    "                                     UMazeGenerator, TMazeGenerator, WaterMazeGenerator\n",
    "from gym_maze.envs.Astar_solver import AstarSolver\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import random\n",
    "import imageio\n",
    "from gym_maze.envs import MazeEnv  # 假设 maze.py 文件在同一目录下\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "class myMazeEnv(MazeEnv):\n",
    "    def __init__(self, maze_generator, action_type='VonNeumann', render_trace=False):\n",
    "        super(myMazeEnv, self).__init__(maze_generator, action_type=action_type, render_trace=render_trace)\n",
    "        \n",
    "        # 定义新的观察空间，前后左右四个方向，每个方向有4种可能：墙、路、终点\n",
    "        self.observation_space = spaces.MultiDiscrete([4, 4, 4, 4])\n",
    "\n",
    "    def _get_observation(self):\n",
    "        # 获取前后左右四个方向的状态\n",
    "        # 0: 墙, 1: 路, 2: 终点\n",
    "        observation = []\n",
    "        for direction in ['up', 'down', 'left', 'right']:\n",
    "            observation.append(self._check_direction(direction))\n",
    "        return np.array(observation)\n",
    "\n",
    "    def _check_direction(self, direction):\n",
    "        # 检查指定方向的状态\n",
    "        # 返回0（墙），1（路）或2（终点）\n",
    "        x, y = self.state\n",
    "        if direction == 'up':\n",
    "            new_x, new_y = x - 1, y\n",
    "        elif direction == 'down':\n",
    "            new_x, new_y = x + 1, y\n",
    "        elif direction == 'left':\n",
    "            new_x, new_y = x, y - 1\n",
    "        elif direction == 'right':\n",
    "            new_x, new_y = x, y + 1\n",
    "        else:\n",
    "            raise ValueError(\"Invalid direction\")\n",
    "\n",
    "        if new_x < 0 or new_x >= self.maze.shape[0] or new_y < 0 or new_y >= self.maze.shape[1]:\n",
    "            return 0  # 墙\n",
    "        elif (new_x, new_y) in self.goal_states:\n",
    "            return 2  # 终点\n",
    "        elif self.maze[new_x, new_y] == 1:\n",
    "            return 0  # 墙\n",
    "        else:\n",
    "            return 1  # 路\n",
    "        \n",
    "        # 初始化迷宫环境\n",
    "maze = RandomMazeGenerator(width=25, height=15, complexity=.5, density=.5)\n",
    "env = myMazeEnv(maze, action_type='VonNeumann', render_trace=False)\n",
    "#env = MazeEnv(maze, action_type='VonNeumann', render_trace=False)\n",
    "env.reset()\n",
    "env.init_state = (13, 23)\n",
    "env.state = env.init_state\n",
    "env.traces = [env.init_state]\n",
    "env.goal_states = [(1, 1)]\n",
    "\n",
    "print(\"Observation Space\", env.observation_space)\n",
    "#print(\"Sample observation\", env.observation_space.sample())  # 显示一个随机的观察\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "#print(\"Action Space Sample\", env.action_space.sample())\n",
    "#state_space = env.observation_space\n",
    "state_space = env.observation_space\n",
    "print(\"There are \", state_space, \" possible states\")\n",
    "\n",
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")\n",
    "\n",
    "def initialize_q_table(state_space, action_space):\n",
    "    # For MultiDiscrete observation space, we need to calculate the total number of states\n",
    "    num_states = np.prod(state_space.nvec)\n",
    "    Qtable = np.zeros((num_states, action_space))\n",
    "    return Qtable\n",
    "\n",
    "Qtable_frozenlake = initialize_q_table(env.observation_space, action_space)\n",
    "\n",
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "  random_int = random.uniform(0,1)\n",
    "  if random_int > epsilon:\n",
    "    action = np.argmax(Qtable[state])\n",
    "  else:\n",
    "    action = env.action_space.sample()\n",
    "  return action\n",
    "\n",
    "def greedy_policy(Qtable, state):\n",
    "  action = np.argmax(Qtable[state])\n",
    "  return action\n",
    "\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "n_training_episodes = 1000\n",
    "learning_rate = 0.7        \n",
    "\n",
    "# Evaluation parameters\n",
    "n_eval_episodes = 100      \n",
    "\n",
    "# Environment parameters\n",
    "env_id = \"FrozenLake-v1\"   \n",
    "max_steps = 99             \n",
    "gamma = 0.95               \n",
    "eval_seed = []             \n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0           \n",
    "min_epsilon = 0.05           \n",
    "decay_rate = 0.0005   \n",
    "     \n",
    "       \n",
    "# Q-learning parameters\n",
    "alpha = 0.1          # Learning rate\n",
    "gamma = 0.9          # Discount factor\n",
    "epsilon = 1.0        # Exploration rate\n",
    "epsilon_decay = 0.995\n",
    "min_epsilon = 0.01\n",
    "num_episodes = 1000  # Number of episodes for training\n",
    "max_steps = 100      # Max steps per episode\n",
    "\n",
    "# Initialize Q-table as a dictionary of dictionaries (for state-action pairs)\n",
    "Q_table = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "# Function to choose an action based on epsilon-greedy policy\n",
    "def choose_action(state):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        # Explore: choose a random action\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        # Exploit: choose the best action based on current Q-table\n",
    "        return np.argmax(Q_table[state])\n",
    "\n",
    "# Q-learning training loop\n",
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable, learning_rate=0.1, gamma=0.99):\n",
    "    for episode in trange(n_training_episodes):\n",
    "        # Decay epsilon\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Choose action according to the epsilon-greedy policy\n",
    "            action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "            \n",
    "            # Take the action and observe the new state and reward\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Update the Q-table using the Q-learning formula\n",
    "            Qtable[state][action] += learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
    "\n",
    "            # Update the state\n",
    "            state = new_state\n",
    "\n",
    "            # If done, finish the episode\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    return Qtable\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd765d9204245d0add332ea31e0a278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import trange\n",
    "\n",
    "trained_Qtable = train(1000, 0.01, 1.0, 0.0005, env, 100, Qtable_frozenlake)\n",
    "print(trained_Qtable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
